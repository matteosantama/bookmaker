{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from typing import Tuple, List, Union\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(_type: str) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    \"\"\"Retrieve X, Y data from the proper directory. You can\n",
    "    specify whether you want it to be pulled from /train, /dev, \n",
    "    or /test, and all the *-data.csv files will be loaded in.\n",
    "    \"\"\"\n",
    "    \n",
    "    if _type not in {'train', 'dev', 'test'}:\n",
    "        msg = f\"{_type} not supported. Try 'train', 'dev', or 'test'.\"\n",
    "        raise RuntimeError(msg)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    data_path = os.path.join('data', _type, '*-data.csv')\n",
    "    for fp in glob.glob(data_path):\n",
    "        # Cast to float because othewise we run into a type\n",
    "        # mismatch error in PyTorch\n",
    "        season_df = pd.read_csv(\n",
    "            fp, index_col=[0, 1], header=[0, 1, 2], dtype='float32')\n",
    "        \n",
    "        df = df.append(season_df)\n",
    "    \n",
    "    features = df[['this', 'other']]\n",
    "\n",
    "    # Normalize features\n",
    "    features = (features - features.mean()) / features.std()\n",
    "    scores = df['TEAM_PTS']\n",
    "    \n",
    "    n_features = len(features.columns)\n",
    "    n_output = len(scores.columns)\n",
    "    msg = 'Uh oh, you might be losing features!'\n",
    "    assert n_features + n_output == len(df.columns), msg\n",
    "    \n",
    "    features = torch.from_numpy(features.values)\n",
    "    scores = torch.from_numpy(scores.values)\n",
    "    \n",
    "    return features, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_two_layers(n: int) -> List[Union[nn.Linear, nn.ReLU]]:\n",
    "    \"\"\"Given an input with n features, construct a series\n",
    "    of neural network layers that decrease logarithmically.\n",
    "    \"\"\"\n",
    "    shift_bit_length = lambda x: 1 << (x - 1).bit_length() - 1\n",
    "    \n",
    "    layers = []\n",
    "    while n > 4:\n",
    "        power_of_two = shift_bit_length(n)\n",
    "        # I belive nn.Linear uses Xavier initialization\n",
    "        layers.append( nn.Linear(n, power_of_two) )\n",
    "        layers.append( nn.ReLU() )\n",
    "        n = power_of_two\n",
    "    # Now n == 4 and we add a final regression layer\n",
    "    layers.append( nn.Linear(n, 1) )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Driver Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(351.3893)\n",
      "1 tensor(263.3325)\n",
      "2 tensor(216.3497)\n",
      "3 tensor(179.8292)\n",
      "4 tensor(177.0203)\n",
      "5 tensor(203.8855)\n",
      "6 tensor(256.8966)\n",
      "7 tensor(201.4151)\n",
      "8 tensor(252.6625)\n",
      "9 tensor(253.8376)\n",
      "10 tensor(162.8323)\n",
      "11 tensor(200.6913)\n",
      "12 tensor(230.3293)\n",
      "13 tensor(227.2189)\n",
      "14 tensor(171.5022)\n",
      "15 tensor(157.9652)\n",
      "16 tensor(228.7696)\n",
      "17 tensor(355.9074)\n",
      "18 tensor(242.6539)\n",
      "19 tensor(179.5560)\n",
      "20 tensor(385.3568)\n",
      "21 tensor(215.0043)\n",
      "22 tensor(344.5336)\n",
      "23 tensor(218.1063)\n",
      "24 tensor(210.0494)\n",
      "25 tensor(205.5783)\n",
      "26 tensor(226.8335)\n",
      "27 tensor(251.2827)\n",
      "28 tensor(198.3258)\n",
      "29 tensor(193.9536)\n",
      "30 tensor(275.4183)\n",
      "31 tensor(245.3987)\n",
      "32 tensor(187.7682)\n",
      "33 tensor(219.7996)\n",
      "34 tensor(197.6419)\n",
      "35 tensor(196.7668)\n",
      "36 tensor(202.6770)\n",
      "37 tensor(181.9888)\n",
      "38 tensor(164.5212)\n",
      "39 tensor(199.4467)\n",
      "40 tensor(187.8617)\n",
      "41 tensor(201.7645)\n",
      "42 tensor(195.3200)\n",
      "43 tensor(268.5247)\n",
      "44 tensor(273.4436)\n",
      "45 tensor(218.6918)\n",
      "46 tensor(257.9191)\n",
      "47 tensor(175.1298)\n",
      "48 tensor(174.3317)\n",
      "49 tensor(213.6090)\n",
      "50 tensor(197.5801)\n",
      "51 tensor(330.7699)\n",
      "52 tensor(186.1796)\n",
      "53 tensor(211.8153)\n",
      "54 tensor(176.5948)\n",
      "55 tensor(179.0541)\n",
      "56 tensor(187.9340)\n",
      "57 tensor(266.9950)\n",
      "58 tensor(201.6700)\n",
      "59 tensor(206.5077)\n",
      "60 tensor(229.0671)\n",
      "61 tensor(247.4780)\n",
      "62 tensor(229.4385)\n",
      "63 tensor(235.9725)\n",
      "64 tensor(209.2532)\n",
      "65 tensor(218.6005)\n",
      "66 tensor(230.0122)\n",
      "67 tensor(271.2003)\n",
      "68 tensor(199.7329)\n",
      "69 tensor(190.5143)\n",
      "70 tensor(256.4446)\n",
      "71 tensor(177.6870)\n",
      "72 tensor(251.2172)\n",
      "73 tensor(309.4515)\n",
      "74 tensor(168.5525)\n",
      "75 tensor(249.8199)\n",
      "76 tensor(202.1628)\n",
      "77 tensor(185.8538)\n",
      "78 tensor(187.1017)\n",
      "79 tensor(257.6434)\n",
      "80 tensor(189.6055)\n",
      "81 tensor(242.2557)\n",
      "82 tensor(190.7165)\n",
      "83 tensor(249.7959)\n",
      "84 tensor(198.0408)\n",
      "85 tensor(210.4768)\n",
      "86 tensor(208.0335)\n",
      "87 tensor(246.3826)\n",
      "88 tensor(354.7888)\n",
      "89 tensor(243.4782)\n",
      "90 tensor(205.5674)\n",
      "91 tensor(237.4781)\n",
      "92 tensor(205.0639)\n",
      "93 tensor(172.7927)\n",
      "94 tensor(376.6640)\n",
      "95 tensor(208.4744)\n",
      "96 tensor(303.7871)\n",
      "97 tensor(212.0601)\n",
      "98 tensor(219.9996)\n",
      "99 tensor(217.7280)\n",
      "100 tensor(221.2608)\n",
      "101 tensor(195.8643)\n",
      "102 tensor(210.5822)\n",
      "103 tensor(203.8808)\n",
      "104 tensor(175.3274)\n",
      "105 tensor(240.2115)\n",
      "106 tensor(190.5919)\n",
      "107 tensor(220.2676)\n",
      "108 tensor(222.6196)\n",
      "109 tensor(193.2739)\n",
      "110 tensor(265.2659)\n",
      "111 tensor(196.1176)\n",
      "112 tensor(237.7683)\n",
      "113 tensor(226.6945)\n",
      "114 tensor(211.2299)\n",
      "115 tensor(199.2888)\n",
      "116 tensor(269.9684)\n",
      "117 tensor(192.2457)\n",
      "118 tensor(283.0014)\n",
      "119 tensor(230.5551)\n",
      "120 tensor(217.2656)\n",
      "121 tensor(292.7862)\n",
      "122 tensor(224.0853)\n",
      "123 tensor(191.6228)\n",
      "124 tensor(260.9940)\n",
      "125 tensor(237.0039)\n",
      "126 tensor(204.2041)\n",
      "127 tensor(259.6410)\n",
      "128 tensor(214.8504)\n",
      "129 tensor(223.8634)\n",
      "130 tensor(261.0176)\n",
      "131 tensor(195.9889)\n",
      "132 tensor(186.7131)\n",
      "133 tensor(179.2432)\n",
      "134 tensor(211.3240)\n",
      "135 tensor(325.1985)\n",
      "136 tensor(211.5921)\n",
      "137 tensor(214.1201)\n",
      "138 tensor(178.7917)\n",
      "139 tensor(260.0597)\n",
      "140 tensor(186.3660)\n",
      "141 tensor(213.9549)\n",
      "142 tensor(219.2323)\n",
      "143 tensor(187.0897)\n",
      "144 tensor(208.1839)\n",
      "145 tensor(213.3008)\n",
      "146 tensor(210.6695)\n",
      "147 tensor(256.1185)\n",
      "148 tensor(200.6324)\n",
      "149 tensor(214.3930)\n",
      "150 tensor(240.1591)\n",
      "151 tensor(200.7857)\n",
      "152 tensor(226.1187)\n",
      "153 tensor(235.8365)\n",
      "154 tensor(236.9294)\n",
      "155 tensor(190.5500)\n",
      "156 tensor(179.8038)\n",
      "157 tensor(242.3088)\n",
      "158 tensor(187.6515)\n",
      "159 tensor(239.3887)\n",
      "160 tensor(250.8094)\n",
      "161 tensor(257.0406)\n",
      "162 tensor(209.1012)\n",
      "163 tensor(217.0865)\n",
      "164 tensor(293.9261)\n",
      "165 tensor(281.0928)\n",
      "166 tensor(300.6687)\n",
      "167 tensor(221.8036)\n",
      "168 tensor(276.6410)\n",
      "169 tensor(276.6733)\n",
      "170 tensor(200.3053)\n",
      "171 tensor(281.5703)\n",
      "172 tensor(252.7148)\n",
      "173 tensor(232.0632)\n",
      "174 tensor(211.9053)\n",
      "175 tensor(310.6494)\n",
      "176 tensor(251.9854)\n",
      "177 tensor(273.3882)\n",
      "178 tensor(190.3965)\n",
      "179 tensor(263.4640)\n",
      "180 tensor(338.6217)\n",
      "181 tensor(222.9052)\n",
      "182 tensor(242.2345)\n",
      "183 tensor(199.7968)\n",
      "184 tensor(198.3350)\n",
      "185 tensor(274.6907)\n",
      "186 tensor(230.9531)\n",
      "187 tensor(276.4587)\n",
      "188 tensor(220.5909)\n",
      "189 tensor(212.0347)\n",
      "190 tensor(235.1714)\n",
      "191 tensor(216.8497)\n",
      "192 tensor(196.1724)\n",
      "193 tensor(211.8087)\n",
      "194 tensor(199.8628)\n",
      "195 tensor(235.8153)\n",
      "196 tensor(207.1703)\n",
      "197 tensor(269.4099)\n",
      "198 tensor(241.2092)\n",
      "199 tensor(217.6436)\n"
     ]
    }
   ],
   "source": [
    "# Roughly follows https://pytorch.org/tutorials/beginner/nn_tutorial.html#\n",
    "\n",
    "# Model hyperparameters\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Load data as torch.tensors\n",
    "x_train, y_train = get_data('train')\n",
    "x_validate, y_validate = get_data('dev')\n",
    "\n",
    "# Define our model layers by decreasing powers of two\n",
    "model = nn.Sequential( *log_two_layers(x_train.shape[1]) )\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validate_ds = TensorDataset(x_validate, y_validate)\n",
    "validate_dl = DataLoader(validate_ds, batch_size=batch_size * 2)\n",
    "\n",
    "# L1 loss is more robust to outliers\n",
    "# loss_func = F.l1_loss\n",
    "loss_func = F.mse_loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        validate_loss = sum(loss_func(model(xb), yb) for xb, yb in validate_dl)\n",
    "\n",
    "    # Print epoch number and average validation loss\n",
    "    print(epoch, validate_loss / len(validate_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 98.7893],\n",
       "        [107.7839],\n",
       "        [ 94.6973],\n",
       "        ...,\n",
       "        [ 99.2690],\n",
       "        [116.1662],\n",
       "        [ 98.9562]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test, y_test = get_data('test')\n",
    "\n",
    "y_predicted = model(x_test)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
